# Dictionary Writing System Specification

## 1. Introduction

### 1.1 Purpose

This document outlines the specifications for a Flask-based Dictionary Writing System (DWS) designed to interact with a BaseX XML database for managing large-scale lexicographic data in the LIFT format. The system aims to provide a fast, mobile-friendly interface for dictionary management with extensive customization options and advanced linguistic functionalities.

### 1.2 Background

The current workflow relies on SIL Fieldworks Explorer (Flex), which has become inadequate for managing a large lexicon containing over 153,000 entries. Performance issues necessitate moving to a dedicated hierarchical database solution with optimized operations for large datasets.

### 1.3 Project Scope

The system will:

- Provide a responsive web interface for dictionary management
- Connect to a BaseX XML database for LIFT file operations
- Support all standard lexicographic operations
- Include specialized tools for linguistic analysis and data enrichment
- Enable batch import/export functionality
- Support semantic relation management
- Allow extensive customization of input and output

## 2. System Architecture

### 2.1 Overview

The system will be built using a three-tier architecture:

1. **Frontend**: Flask-based responsive web application
2. **Backend**: Python API for business logic and data processing
3. **Database**: BaseX XML database for storing the LIFT format dictionary

### 2.2 Technology Stack

- **Frontend**: Flask, JavaScript, Bootstrap (for responsive design)
- **Backend**: Python 3.x
- **Database**: BaseX XML database
- **API**: RESTful API for communication between frontend and backend
- **Authentication**: JWT-based authentication

### 2.3 System Components

1. **User Interface Layer**
   - Responsive web interface
   - Mobile-friendly design
   - Customizable layouts

2. **Application Layer**
   - Dictionary management services
   - Import/Export services
   - Analysis tools
   - Search and filter services

3. **Data Access Layer**
   - BaseX XML database connector
   - LIFT format parser/generator
   - Cache management for improved performance

## 3. Functional Requirements

### 3.1 Dictionary Management

#### 3.1.1 Entry Management

- Create, read, update, and delete dictionary entries
- Support for all LIFT format features and attributes
- Bulk operations for multiple entries
- Version history and change tracking
- Support for multimedia attachments (images, audio)

#### 3.1.2 Semantic Relations

- Define and manage semantic relations between entries
- Support for hierarchical relations (hypernyms, hyponyms)
- Support for associative relations (synonyms, antonyms, etc.)
- Visual representation of semantic networks
- Circular reference detection and prevention

#### 3.1.3 Grammatical Information

- Support for all grammatical categories defined in the LIFT ranges
- Customizable part-of-speech hierarchies
- Morphological analysis and generation
- Cross-linguistic grammatical mapping
- **Noun Countability Classification**:
  - Integration with trained countability models
  - Automatic suggestion of countability for nouns
  - Countability tags for dictionary entries (countable, uncountable, both)
  - Statistical confidence scoring for suggested classifications

#### 3.1.4 Pronunciation Management

- **IPA Pronunciation**:
  - Storage and management of IPA transcriptions
  - Validation against admissible character sets and patterns
  - Support for British and American pronunciation variants
  - Bulk pronunciation update and validation

- **Text-to-Speech Integration**:
  - Google Cloud TTS integration for audio generation
  - Local storage of generated audio files
  - Batch processing for pronunciation audio
  - Audio playback in the UI

- **Pronunciation Validation**:
  - Character-level validation against IPA standard
  - Sequence validation for admissible character pairs
  - Error detection and suggestion system
  - Visual highlighting of problematic sequences

#### 3.1.5 Examples and Sense Management

- **Example Association**:
  - Automated tools for attaching standalone examples to appropriate senses
  - Batch processing for example reorganization
  - NLP-based example classification to suggest appropriate sense mappings
  - Tracking of orphaned examples and suggestions for incorporation

- **Sense Mapping and Completeness**:
  - WordNet sense mapping for completeness verification
  - Automatic detection of missing senses compared to reference resources
  - Sense alignment tools with statistical confidence scoring
  - Sense hierarchy visualization and management

- **LLM-Assisted Example Organization**:
  - Specialized LLM-based tools for sense disambiguation
  - Example context analysis to determine appropriate sense attachment
  - Batch verification of example-to-sense mapping accuracy
  - Interactive review interface for ambiguous cases

### 3.2 Search and Browse

#### 3.2.1 Basic Search

- Full-text search across all fields
- Advanced filtering by field values
- Support for regular expressions
- Phonetic search capabilities

#### 3.2.2 Advanced Search

- Compound search with multiple criteria
- Search within search results
- Save and load search queries
- Export search results

#### 3.2.3 Browse Interface

- Alphabetical browsing
- Browsing by semantic domain
- Browsing by grammatical category
- Customizable browse views

### 3.3 Data Import/Export

#### 3.3.1 Import Capabilities

- Import from LIFT format
- Import from custom YAML format
- Import from JSON format
- Import from SFM format
- Validation of imported data
- Circular reference detection and resolution

#### 3.3.2 Export Capabilities

- **Kindle Dictionary Export**:
  - Generation of Kindle-compatible dictionary format (.opf, .mobi)
  - Support for Kindle indexing features (inflection forms)
  - Custom formatting and styling options
  - Automatic generation of front and back matter
  - Pronunciation guides using IPA notation
  - Cover image and metadata customization

- **Flutter Mobile App Export**:
  - SQLite database generation optimized for mobile performance
  - Compression of data for smaller app footprint
  - Indexing structure for fast mobile search
  - Support for offline usage and incremental updates
  - Schema designed for Flutter application compatibility

- **Standard Export Formats**:
  - Export to LIFT format for interoperability
  - Export to custom formats (YAML, JSON, TSV)
  - Selective export based on criteria
  - Export templates for different purposes

#### 3.3.3 Batch Processing

- Scheduled batch operations
- Progress tracking for long-running operations
- Error handling and reporting
- Automated validation before and after processing

### 3.4 Analysis Tools

#### 3.4.1 Duplicate Detection

- Multi-criteria duplicate finding
- Configurable similarity thresholds
- Batch merge operations

#### 3.4.2 Statistical Analysis

- Frequency analysis
- Anomaly detection
- Distribution reports
- Completeness assessment

### 3.4.3 Linguistic Analysis

- **Pronunciation Modeling**:
  - IPA transcription generation using transformer models
  - Validation against phonological rules
  - Integration with Google Cloud TTS for audio verification
  - Storage and management of pronunciation data separate from core dictionary

- **POS and Grammatical Analysis**:
  - POS tagging and verification
  - Noun countability classification using trained models
  - Grammatical feature prediction
  - Cross-linguistic feature mapping

- **Semantic Analysis**:
  - Example sentence analysis
  - Compound word analysis
  - Sense disambiguation
  - Usage pattern detection

### 3.5 Integration with LLMs

#### 3.5.1 LLM-Generated Content

- Example sentence generation
- Definition enhancement
- Translation suggestions
- Semantic domain classification

#### 3.5.2 LLM Integration

- **API Integration**:
  - Integration with OpenAI and other LLM providers
  - Local models support for privacy-sensitive operations
  - Batch processing of entries with LLMs
  - Optimized token usage with customized prompting strategies

- **Example and Sense Processing**:
  - Specialized prompting for example-to-sense allocation
  - Sense disambiguation capabilities
  - Customized formatting for different LLM services
  - Confidence scoring for suggested assignments

- **Resource-Aware Processing**:
  - Offline batch processing for computationally intensive tasks
  - Queue management for LLM API calls
  - Local execution options for resource-intensive operations
  - API throttling and cost management strategies

## 4. Non-Functional Requirements

### 4.1 Performance

#### 4.1.1 Response Time

- Page load time under 2 seconds
- Search results returned in under 1 second
- Bulk operations optimized for large datasets

#### 4.1.2 Scalability

- Support for dictionaries with 200,000+ entries
- Horizontal scaling capabilities
- Caching mechanisms for frequently accessed data

### 4.2 Usability

#### 4.2.1 User Interface

- Intuitive navigation
- Customizable layouts
- Keyboard shortcuts for common operations
- Dark/light mode support

#### 4.2.2 Accessibility

- WCAG 2.1 AA compliance
- Screen reader compatibility
- Support for high-contrast modes

### 4.3 Security

#### 4.3.1 Authentication and Authorization

- Role-based access control
- Secure authentication
- Session management

#### 4.3.2 Data Protection

- Encrypted data storage
- Regular automated backups
- Audit logging of all changes

### 4.4 Reliability

#### 4.4.1 Availability

- 99.9% uptime
- Graceful degradation under load
- Automatic recovery from failures

#### 4.4.2 Data Integrity

- Transaction support for all operations
- Validation of all user inputs
- Conflict resolution for concurrent edits

#### 4.4.3 Backup and Rollback

- **Comprehensive Backup System**:
  - Automated incremental backups of the entire database
  - Configurable backup schedule (hourly, daily, weekly)
  - Backup versioning with retention policies
  - Compression and encryption options for backups
  - External storage support (cloud, network drives)

- **Fine-grained Rollback Capabilities**:
  - Transaction-level rollback for individual operations
  - Session-level rollback for user editing sessions
  - Point-in-time recovery options
  - Selective rollback for specific entries or changes
  - Visual diff and merge tools for resolving conflicts

- **Audit and Recovery**:
  - Complete audit trail of all changes
  - User activity logging
  - Change history visualization
  - Disaster recovery procedures
  - Testing and verification of backup integrity

## 5. Database Design

### 5.1 BaseX Configuration

BaseX is an XML database management system optimized for storing, querying, and managing hierarchical XML data, making it ideal for LIFT format dictionaries. The configuration will include:

- **Optimized XML Indexing**:
  - Value indexes for fast text-based searches
  - Full-text indexes with custom tokenization for linguistic searches
  - Path indexes for efficient XPath/XQuery performance
  - Custom indexes for frequently accessed elements (e.g., headwords, parts of speech)

- **Performance Tuning**:
  - Database splitting by initial letters to improve query performance on large datasets
  - Memory allocation optimization for handling 200,000+ entries
  - Query optimization and caching for common search patterns
  - Compression settings to reduce storage requirements while maintaining performance

- **Concurrency Management**:
  - Lock management for multi-user editing scenarios
  - Transaction isolation levels to prevent data corruption
  - Connection pooling for efficient resource utilization

- **Integration Features**:
  - REST API configuration for external access
  - WebDAV for alternative file access
  - XSLT processing for transformation tasks

### 5.2 LIFT Schema Integration

- Support for standard LIFT schema
- Custom extensions for project-specific needs
- Schema validation for data integrity
- Automated validation against the LIFT schema during import and update operations

### 5.3 Caching Strategy

- In-memory caching for frequently accessed data
- Query result caching
- Cache invalidation strategies
- Tiered caching architecture (memory, disk, distributed)

## 6. User Interface Design

### 6.1 Layout

- Responsive design for all screen sizes
- Split-pane interface for efficient editing
- Collapsible panels for advanced features

### 6.2 Entry Editor

- Rich text editing capabilities
- Inline validation of entries
- Auto-save functionality
- Side-by-side comparison view

### 6.3 Search Interface

- Instant search results
- Faceted search navigation
- Visual query builder
- Search history

### 6.4 Dashboard

- Customizable widgets
- Progress tracking
- Recent activities
- System status

## 7. API Design

### 7.1 RESTful Endpoints

#### 7.1.1 Core API Endpoints

- **Entry Management**:
  - `GET /api/entries` - List entries with filtering options
  - `GET /api/entries/{id}` - Get specific entry
  - `POST /api/entries` - Create new entry
  - `PUT /api/entries/{id}` - Update existing entry
  - `DELETE /api/entries/{id}` - Delete entry

- **Search and Filter**:
  - `GET /api/search` - Full-text search with advanced parameters
  - `GET /api/browse/{criteria}` - Browse by alphabet, category, etc.
  - `GET /api/filter` - Filter entries by multiple criteria

- **Batch Operations**:
  - `POST /api/batch/import` - Import batch of entries
  - `GET /api/batch/export` - Export selected entries
  - `POST /api/batch/update` - Update multiple entries

#### 7.1.2 Specialized API Endpoints

- **Pronunciation Services**:
  - `POST /api/pronunciation/tts` - Generate TTS audio using Google Cloud
  - `GET /api/pronunciation/{word}` - Get existing pronunciation data
  - `PUT /api/pronunciation/{word}` - Update pronunciation
  - `POST /api/pronunciation/validate` - Validate IPA pronunciation against rules

- **External Services Integration**:
  - `POST /api/services/google-cloud-tts` - Direct interface to Google Cloud TTS
  - `GET /api/services/models/countability` - Access trained countability model
  - `POST /api/services/analyze` - Process text with NLP services

- **Advanced Linguistic Services**:
  - `POST /api/linguistic/relations` - Manage semantic relations
  - `GET /api/linguistic/analysis/{type}` - Get linguistic analysis
  - `POST /api/linguistic/validation` - Validate linguistic data

### 7.2 Authentication

- JWT-based authentication
- API key management
- Rate limiting

### 7.3 Documentation

- Interactive API documentation (Swagger/OpenAPI)
- Code examples
- SDKs for common languages

## 8. Deployment

### 8.1 Installation

- Docker containerization
- Dependency management
- Configuration management
- Oracle Free Tier deployment guidelines

### 8.2 Infrastructure

- **Oracle Free Tier Optimization**:
  - Resource allocation strategies for BaseX and Flask
  - Memory management for constrained environments
  - Performance tuning for Oracle Cloud infrastructure
  - Caching strategies to minimize computational costs

- **Distributed Processing**:
  - Offloading computationally intensive tasks to local environment
  - API design for hybrid cloud/local processing
  - Asynchronous job processing for resource-intensive operations
  - Result synchronization between cloud and local environments

### 8.3 Updates

- Rolling updates without downtime
- Database migration strategies
- Backwards compatibility

### 8.3 Monitoring

- Performance monitoring
- Error tracking
- Usage analytics

## 9. Migration Strategy

### 9.1 Data Migration

- Incremental migration from Flex
- Data validation during migration
- Rollback capabilities

### 9.2 Process Migration

- Parallel operation during transition
- User training
- Gradual feature adoption

## 10. Integration with Existing Tools

### 10.1 Flex Integration

- Import/export compatibility
- Synchronization options
- Feature parity assessment

### 10.2 Script Adaptation

- **Porting of Existing Python Scripts**:
  - Migration of FLExTools scripts to the new system architecture
  - Adaptation from Flex object model to LIFT/BaseX data model
  - Performance optimization of existing algorithms
  - Integration with the new UI framework

- **Enhanced Validation**:
  - Complex validation rules implementation (circular references, consistency checks)
  - Statistical anomaly detection
  - Linguistic pattern verification
  - Cross-reference integrity checking

- **Advanced Processing**:
  - Integration of existing NLP pipelines
  - Adaptation of pronunciation modeling tools
  - Porting of example association algorithms
  - Enhanced compound analysis tools

### 10.3 Examples and Senses Migration

- **Example Association Tools**:
  - Conversion of standalone examples to sense-attached examples
  - Batch processing tools for large-scale example reorganization
  - Statistical models for assigning examples to appropriate senses
  - Progress tracking and validation for migration completeness

- **WordNet Integration**:
  - Mapping of dictionary senses to WordNet synsets
  - Gap identification for missing senses
  - Automated suggestions for sense hierarchy organization
  - Verification tools for semantic coverage

## 11. Future Enhancements

### 11.1 Collaboration Features

- Multi-user editing
- Commenting and discussion
- Workflow management

### 11.2 Advanced Analytics

- Machine learning for anomaly detection
- Pattern recognition in language data
- Automatic relation suggestion

### 11.3 Publishing

- Additional publishing formats beyond Kindle and Flutter
- Print-ready PDF output
- Web dictionary generation
- Integration with third-party publishing platforms

## 12. Implementation Plan

### 12.1 Phase 1: Core Functionality

- Database setup and configuration
- Basic CRUD operations
- Simple search functionality
- User authentication

### 12.2 Phase 2: Advanced Features

- Advanced search and filtering
- Batch operations
- Analysis tools
- Import/export capabilities

### 12.3 Phase 3: Optimization and Enhancement

- Performance tuning
- Mobile optimization
- LLM integration
- Advanced customization

## 13. Glossary

- **LIFT**: Lexicon Interchange Format, an XML standard for lexical data
- **Flex**: FieldWorks Language Explorer, a tool for language documentation
- **BaseX**: XML database optimized for hierarchical data
- **LLM**: Large Language Model

## 14. Appendices

### 14.1 LIFT Format Reference

- XML schema
- Element descriptions
- Range definitions

### 14.2 Example API Calls

- Entry creation
- Search operations
- Batch processing

### 14.4 IPA Character Sets and Validation Rules

The following defines the admissible IPA characters and sequences for pronunciation validation:

#### 14.4.1 Primary IPA Symbols

- Vowels: `ɑ æ ɒ ə ɜ ɪ i ʊ u ʌ e ɛ o ɔ`
- Length markers: `ː`
- Consonants: `b d f g h j k l m n p r s t v w z ð θ ŋ ʃ ʒ tʃ dʒ`
- Stress markers: `ˈ ˌ`
- Special symbols: `ᵻ`

#### 14.4.2 Valid Diphthongs

- `eɪ aɪ ɔɪ əʊ aʊ ɪə eə ʊə oʊ`

#### 14.4.3 Invalid Character Sequences

- Double stress markers: `ˈˈ ˌˌ ˈˌ ˌˈ`
- Invalid consonant clusters: [list to be compiled from data]
- Other phonotactic constraints specific to English

#### 14.4.4 Dialect-Specific Rules

- British English specific patterns
- American English specific patterns
- Allowable dialectal variations
